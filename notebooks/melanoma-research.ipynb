{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":["!pip install -q efficientnet >> /dev/null"],"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["import pandas as pd, numpy as np\n","from kaggle_datasets import KaggleDatasets\n","import tensorflow as tf, re, math\n","import tensorflow.keras.backend as K\n","import efficientnet.tfkeras as efn\n","from sklearn.model_selection import KFold\n","from sklearn.metrics import roc_auc_score\n","import matplotlib.pyplot as plt\n","from functools import partial"],"metadata":{"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":["## Configuration\n","In order to be a proper cross validation with a meaningful overall CV score (aligned with LB score), **you need to choose the same** `IMG_SIZES`, `INC2019`, `INC2018`, and `EFF_NETS` **for each fold**. If your goal is to just run lots of experiments, then you can choose to have a different experiment in each fold. Then each fold is like a holdout validation experiment. When you find a configuration you like, you can use that configuration for all folds. \n","* DEVICE - is GPU or TPU\n","* SEED - a different seed produces a different triple stratified kfold split.\n","* FOLDS - number of folds. Best set to 3, 5, or 15 but can be any number between 2 and 15\n","* IMG_SIZES - is a Python list of length FOLDS. These are the image sizes to use each fold\n","* INC2019 - This includes the new half of the 2019 competition data. The second half of the 2019 data is the comp data from 2018 plus 2017\n","* INC2018 - This includes the second half of the 2019 competition data which is the comp data from 2018 plus 2017\n","* BATCH_SIZES - is a list of length FOLDS. These are batch sizes for each fold. For maximum speed, it is best to use the largest batch size your GPU or TPU allows.\n","* EPOCHS - is a list of length FOLDS. These are maximum epochs. Note that each fold, the best epoch model is saved and used. So if epochs is too large, it won't matter.\n","* EFF_NETS - is a list of length FOLDS. These are the EfficientNets to use each fold. The number refers to the B. So a number of `0` refers to EfficientNetB0, and `1` refers to EfficientNetB1, etc.\n","* WGTS - this should be `1/FOLDS` for each fold. This is the weight when ensembling the folds to predict the test set. If you want a weird ensemble, you can use different weights.\n","* TTA - test time augmentation. Each test image is randomly augmented and predicted TTA times and the average prediction is used. TTA is also applied to OOF during validation."],"metadata":{}},{"cell_type":"code","source":["DEVICE = \"TPU\" #or \"GPU\"\n","\n","# USE DIFFERENT SEED FOR DIFFERENT STRATIFIED KFOLD\n","SEED = 42\n","\n","# NUMBER OF FOLDS. USE 3, 5, OR 15 \n","FOLDS = 3\n","\n","# WHICH IMAGE SIZES TO LOAD EACH FOLD\n","# CHOOSE 128, 192, 256, 384, 512, 768 \n","IMG_SIZES = [384,384,384]\n","\n","# INCLUDE OLD COMP DATA? YES=1 NO=0\n","INC2019 = [0,0,0]\n","INC2018 = [1,1,1]\n","\n","# BATCH SIZE AND EPOCHS\n","BATCH_SIZES = [32]*FOLDS\n","EPOCHS = [15]*FOLDS\n","\n","# WEIGHTS FOR FOLD MODELS WHEN PREDICTING TEST\n","WGTS = [1/FOLDS]*FOLDS\n","\n","# TEST TIME AUGMENTATION STEPS\n","TTA = 11\n","\n","# WHETHER TO INCLUDE METADATA OR NOT\n","IMG_ONLY = True"],"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["if DEVICE == \"TPU\":\n","    print(\"connecting to TPU...\")\n","    try:\n","        tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n","        print('Running on TPU ', tpu.master())\n","    except ValueError:\n","        print(\"Could not connect to TPU\")\n","        tpu = None\n","\n","    if tpu:\n","        try:\n","            print(\"initializing  TPU ...\")\n","            tf.config.experimental_connect_to_cluster(tpu)\n","            tf.tpu.experimental.initialize_tpu_system(tpu)\n","            strategy = tf.distribute.experimental.TPUStrategy(tpu)\n","            print(\"TPU initialized\")\n","        except _:\n","            print(\"failed to initialize TPU\")\n","    else:\n","        DEVICE = \"GPU\"\n","\n","if DEVICE != \"TPU\":\n","    print(\"Using default strategy for CPU and single GPU\")\n","    strategy = tf.distribute.get_strategy()\n","\n","if DEVICE == \"GPU\":\n","    print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n","    \n","\n","AUTO     = tf.data.experimental.AUTOTUNE\n","REPLICAS = strategy.num_replicas_in_sync\n","print(f'REPLICAS: {REPLICAS}')"],"metadata":{"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"connecting to TPU...\nRunning on TPU  grpc://10.0.0.2:8470\ninitializing  TPU ...\nTPU initialized\nREPLICAS: 8\n","output_type":"stream"}]},{"cell_type":"markdown","source":["# Step 1: Preprocess\n","Preprocess has already been done and saved to TFRecords. Here we choose which size to load. We can use either 128x128, 192x192, 256x256, 384x384, 512x512, 768x768 by changing the `IMG_SIZES` variable in the preceeding code section. These TFRecords are discussed [here][1]. The advantage of using different input sizes is discussed [here][2]\n","\n","[1]: https://www.kaggle.com/c/siim-isic-melanoma-classification/discussion/155579\n","[2]: https://www.kaggle.com/c/siim-isic-melanoma-classification/discussion/160147"],"metadata":{}},{"cell_type":"code","source":["DATA_PATH = [None]*FOLDS; DATA_PATH2 = [None]*FOLDS\n","for i,k in enumerate(IMG_SIZES):\n","    DATA_PATH[i] = KaggleDatasets().get_gcs_path('melanoma-%ix%i'%(k,k))\n","    DATA_PATH2[i] = KaggleDatasets().get_gcs_path('isic2019-%ix%i'%(k,k))\n","files_train = np.sort(np.array(tf.io.gfile.glob(DATA_PATH[0] + '/train*.tfrec')))\n","files_test  = np.sort(np.array(tf.io.gfile.glob(DATA_PATH[0] + '/test*.tfrec')))"],"metadata":{"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["DATA_PATH2"],"metadata":{"trusted":true},"execution_count":7,"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"['gs://kds-daa129e989ec157e205de7addb63ac52a6bb066eb3efea7e285641dd',\n 'gs://kds-daa129e989ec157e205de7addb63ac52a6bb066eb3efea7e285641dd',\n 'gs://kds-daa129e989ec157e205de7addb63ac52a6bb066eb3efea7e285641dd']"},"metadata":{}}]},{"cell_type":"code","source":["files_train"],"metadata":{"trusted":true},"execution_count":8,"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"array(['gs://kds-e587a221f0797cf8f017657b99d05aee1f262e04db020e07144fa268/train00-2182.tfrec',\n       'gs://kds-e587a221f0797cf8f017657b99d05aee1f262e04db020e07144fa268/train01-2185.tfrec',\n       'gs://kds-e587a221f0797cf8f017657b99d05aee1f262e04db020e07144fa268/train02-2193.tfrec',\n       'gs://kds-e587a221f0797cf8f017657b99d05aee1f262e04db020e07144fa268/train03-2182.tfrec',\n       'gs://kds-e587a221f0797cf8f017657b99d05aee1f262e04db020e07144fa268/train04-2167.tfrec',\n       'gs://kds-e587a221f0797cf8f017657b99d05aee1f262e04db020e07144fa268/train05-2171.tfrec',\n       'gs://kds-e587a221f0797cf8f017657b99d05aee1f262e04db020e07144fa268/train06-2175.tfrec',\n       'gs://kds-e587a221f0797cf8f017657b99d05aee1f262e04db020e07144fa268/train07-2174.tfrec',\n       'gs://kds-e587a221f0797cf8f017657b99d05aee1f262e04db020e07144fa268/train08-2177.tfrec',\n       'gs://kds-e587a221f0797cf8f017657b99d05aee1f262e04db020e07144fa268/train09-2178.tfrec',\n       'gs://kds-e587a221f0797cf8f017657b99d05aee1f262e04db020e07144fa268/train10-2174.tfrec',\n       'gs://kds-e587a221f0797cf8f017657b99d05aee1f262e04db020e07144fa268/train11-2176.tfrec',\n       'gs://kds-e587a221f0797cf8f017657b99d05aee1f262e04db020e07144fa268/train12-2198.tfrec',\n       'gs://kds-e587a221f0797cf8f017657b99d05aee1f262e04db020e07144fa268/train13-2186.tfrec',\n       'gs://kds-e587a221f0797cf8f017657b99d05aee1f262e04db020e07144fa268/train14-2174.tfrec'],\n      dtype='<U84')"},"metadata":{}}]},{"cell_type":"code","source":["files_test"],"metadata":{"trusted":true},"execution_count":9,"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"array(['gs://kds-e587a221f0797cf8f017657b99d05aee1f262e04db020e07144fa268/test00-687.tfrec',\n       'gs://kds-e587a221f0797cf8f017657b99d05aee1f262e04db020e07144fa268/test01-687.tfrec',\n       'gs://kds-e587a221f0797cf8f017657b99d05aee1f262e04db020e07144fa268/test02-687.tfrec',\n       'gs://kds-e587a221f0797cf8f017657b99d05aee1f262e04db020e07144fa268/test03-687.tfrec',\n       'gs://kds-e587a221f0797cf8f017657b99d05aee1f262e04db020e07144fa268/test04-687.tfrec',\n       'gs://kds-e587a221f0797cf8f017657b99d05aee1f262e04db020e07144fa268/test05-687.tfrec',\n       'gs://kds-e587a221f0797cf8f017657b99d05aee1f262e04db020e07144fa268/test06-687.tfrec',\n       'gs://kds-e587a221f0797cf8f017657b99d05aee1f262e04db020e07144fa268/test07-687.tfrec',\n       'gs://kds-e587a221f0797cf8f017657b99d05aee1f262e04db020e07144fa268/test08-687.tfrec',\n       'gs://kds-e587a221f0797cf8f017657b99d05aee1f262e04db020e07144fa268/test09-687.tfrec',\n       'gs://kds-e587a221f0797cf8f017657b99d05aee1f262e04db020e07144fa268/test10-687.tfrec',\n       'gs://kds-e587a221f0797cf8f017657b99d05aee1f262e04db020e07144fa268/test11-687.tfrec',\n       'gs://kds-e587a221f0797cf8f017657b99d05aee1f262e04db020e07144fa268/test12-687.tfrec',\n       'gs://kds-e587a221f0797cf8f017657b99d05aee1f262e04db020e07144fa268/test13-687.tfrec',\n       'gs://kds-e587a221f0797cf8f017657b99d05aee1f262e04db020e07144fa268/test14-687.tfrec',\n       'gs://kds-e587a221f0797cf8f017657b99d05aee1f262e04db020e07144fa268/test15-677.tfrec'],\n      dtype='<U82')"},"metadata":{}}]},{"cell_type":"markdown","source":["# Step 2: Data Augmentation\n","This notebook uses rotation, sheer, zoom, shift augmentation first shown in this notebook [here][1] and successfully used in Melanoma comp by AgentAuers [here][2]. This notebook also uses horizontal flip, hue, saturation, contrast, brightness augmentation similar to last years winner and also similar to AgentAuers' notebook.\n","\n","Additionally we can decide to use external data by changing the variables `INC2019` and `INC2018` in the preceeding code section. These variables respectively indicate whether to load last year 2019 data and/or year 2018 + 2017 data. These datasets are discussed [here][3]\n","\n","Consider experimenting with different augmenation and/or external data. The code to load TFRecords is taken from AgentAuers' notebook [here][2]. Thank you AgentAuers, this is great work.\n","\n","[1]: https://www.kaggle.com/cdeotte/rotation-augmentation-gpu-tpu-0-96\n","[2]: https://www.kaggle.com/agentauers/incredible-tpus-finetune-effnetb0-b6-at-once\n","[3]: https://www.kaggle.com/c/siim-isic-melanoma-classification/discussion/164910"],"metadata":{}},{"cell_type":"code","source":["ROT_ = 180.0\n","SHR_ = 2.0\n","HZOOM_ = 8.0\n","WZOOM_ = 8.0\n","HSHIFT_ = 8.0\n","WSHIFT_ = 8.0"],"metadata":{"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":["def get_mat(rotation, shear, height_zoom, width_zoom, height_shift, width_shift):\n","    # returns 3x3 transformmatrix which transforms indicies\n","        \n","    # CONVERT DEGREES TO RADIANS\n","    rotation = math.pi * rotation / 180.\n","    shear    = math.pi * shear    / 180.\n","\n","    def get_3x3_mat(lst):\n","        return tf.reshape(tf.concat([lst],axis=0), [3,3])\n","    \n","    # ROTATION MATRIX\n","    c1   = tf.math.cos(rotation)\n","    s1   = tf.math.sin(rotation)\n","    one  = tf.constant([1],dtype='float32')\n","    zero = tf.constant([0],dtype='float32')\n","    \n","    rotation_matrix = get_3x3_mat([c1,   s1,   zero, \n","                                   -s1,  c1,   zero, \n","                                   zero, zero, one])    \n","    # SHEAR MATRIX\n","    c2 = tf.math.cos(shear)\n","    s2 = tf.math.sin(shear)    \n","    \n","    shear_matrix = get_3x3_mat([one,  s2,   zero, \n","                                zero, c2,   zero, \n","                                zero, zero, one])        \n","    # ZOOM MATRIX\n","    zoom_matrix = get_3x3_mat([one/height_zoom, zero,           zero, \n","                               zero,            one/width_zoom, zero, \n","                               zero,            zero,           one])    \n","    # SHIFT MATRIX\n","    shift_matrix = get_3x3_mat([one,  zero, height_shift, \n","                                zero, one,  width_shift, \n","                                zero, zero, one])\n","    \n","    return K.dot(K.dot(rotation_matrix, shear_matrix), \n","                 K.dot(zoom_matrix,     shift_matrix))\n","\n","\n","def transform(image, DIM=256):    \n","    # input image - is one image of size [dim,dim,3] not a batch of [b,dim,dim,3]\n","    # output - image randomly rotated, sheared, zoomed, and shifted\n","    XDIM = DIM%2 #fix for size 331\n","    \n","    rot = ROT_ * tf.random.normal([1], dtype='float32')\n","    shr = SHR_ * tf.random.normal([1], dtype='float32') \n","    h_zoom = 1.0 + tf.random.normal([1], dtype='float32') / HZOOM_\n","    w_zoom = 1.0 + tf.random.normal([1], dtype='float32') / WZOOM_\n","    h_shift = HSHIFT_ * tf.random.normal([1], dtype='float32') \n","    w_shift = WSHIFT_ * tf.random.normal([1], dtype='float32') \n","\n","    # GET TRANSFORMATION MATRIX\n","    m = get_mat(rot,shr,h_zoom,w_zoom,h_shift,w_shift) \n","\n","    # LIST DESTINATION PIXEL INDICES\n","    x   = tf.repeat(tf.range(DIM//2, -DIM//2,-1), DIM)\n","    y   = tf.tile(tf.range(-DIM//2, DIM//2), [DIM])\n","    z   = tf.ones([DIM*DIM], dtype='int32')\n","    idx = tf.stack( [x,y,z] )\n","    \n","    # ROTATE DESTINATION PIXELS ONTO ORIGIN PIXELS\n","    idx2 = K.dot(m, tf.cast(idx, dtype='float32'))\n","    idx2 = K.cast(idx2, dtype='int32')\n","    idx2 = K.clip(idx2, -DIM//2+XDIM+1, DIM//2)\n","    \n","    # FIND ORIGIN PIXEL VALUES           \n","    idx3 = tf.stack([DIM//2-idx2[0,], DIM//2-1+idx2[1,]])\n","    d    = tf.gather_nd(image, tf.transpose(idx3))\n","        \n","    return tf.reshape(d,[DIM, DIM,3])"],"metadata":{"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":["def read_tfrecord(example, image_only=True, test_set=False, labeled=True, return_image_names=False):\n","    if not test_set:\n","        if labeled:\n","            tfrec_format = {\n","                'image'                        : tf.io.FixedLenFeature([], tf.string),\n","                'image_name'                   : tf.io.FixedLenFeature([], tf.string),\n","                'patient_id'                   : tf.io.FixedLenFeature([], tf.int64),\n","                'sex'                          : tf.io.FixedLenFeature([], tf.int64),\n","                'age_approx'                   : tf.io.FixedLenFeature([], tf.int64),\n","                'anatom_site_general_challenge': tf.io.FixedLenFeature([], tf.int64),\n","                'target'                       : tf.io.FixedLenFeature([], tf.int64)\n","            }      \n","        else:\n","            tfrec_format = {\n","                'image'                        : tf.io.FixedLenFeature([], tf.string),\n","                'image_name'                   : tf.io.FixedLenFeature([], tf.string),\n","                'patient_id'                   : tf.io.FixedLenFeature([], tf.int64),\n","                'sex'                          : tf.io.FixedLenFeature([], tf.int64),\n","                'age_approx'                   : tf.io.FixedLenFeature([], tf.int64),\n","                'anatom_site_general_challenge': tf.io.FixedLenFeature([], tf.int64),\n","            }\n","    else:\n","        tfrec_format = {\n","                'image'                        : tf.io.FixedLenFeature([], tf.string),\n","                'image_name'                   : tf.io.FixedLenFeature([], tf.string),\n","                'patient_id'                   : tf.io.FixedLenFeature([], tf.int64),\n","                'sex'                          : tf.io.FixedLenFeature([], tf.int64),\n","                'age_approx'                   : tf.io.FixedLenFeature([], tf.int64),\n","                'anatom_site_general_challenge': tf.io.FixedLenFeature([], tf.int64),\n","        }\n","        \n","    example = tf.io.parse_single_example(example, tfrec_format)\n","    if image_only:\n","        if labeled:\n","            return ({\"img_input\": example['image']}, example['target'])\n","        else:\n","            return ({\"img_input\": example['image']}, example['image_name'] if return_image_names else 0)\n","    else:\n","        if not test_set:\n","            if labeled:\n","                return ({\"img_input\": example['image'],\n","                        \"metadata_input\": [example['sex'], example['age_approx'],\n","                                           example['anatom_site_general_challenge']]},\n","                        example['target'])\n","            \n","        return ({\"img_input\": example['image'],\n","                 \"metadata_input\": [example['sex'], example['age_approx'],\n","                                    example['anatom_site_general_challenge']]},\n","                 example['image_name'] if return_image_names else 0)\n","\n","                                                                                                        \n","def prepare_image(features, image_only=True, augment=True, dim=256):  \n","    img = features['img_input']\n","    img = tf.image.decode_jpeg(img, channels=3)\n","    img = tf.cast(img, tf.float32) / 255.0\n","    \n","    if augment:\n","        img = transform(img,DIM=dim)\n","        img = tf.image.random_flip_left_right(img)\n","        #img = tf.image.random_hue(img, 0.01)\n","        img = tf.image.random_saturation(img, 0.7, 1.3)\n","        img = tf.image.random_contrast(img, 0.8, 1.2)\n","        img = tf.image.random_brightness(img, 0.1)\n","                      \n","    img = tf.reshape(img, [dim, dim, 3])\n","    features['img_input'] = img\n","    return features\n","\n","def count_data_items(filenames):\n","    n = [int(re.compile(r\"-([0-9]*)\\.\").search(filename).group(1)) \n","         for filename in filenames]\n","    return np.sum(n)"],"metadata":{"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"markdown","source":["\n","# Step 3: Build Model\n","This is a common model architecute. Consider experimenting with different backbones, custom heads, losses, and optimizers. Also consider inputing meta features into your CNN."],"metadata":{}},{"cell_type":"code","source":["import tensorflow_hub as hub\n","from tensorflow import keras\n","import tensorflow_addons as tfa"],"metadata":{"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":["def build_model(model_name=\"efficientnet_B0\", dim=128, image_only=True,\n","                metadata_length=3, feature_vec_size=1000, normalize_features=True, \n","                return_feature_model=False):\n","    \n","    def get_efficientnet(effnet_model):\n","        effnet_model = int(effnet_model)\n","        EFNS = [efn.EfficientNetB0, efn.EfficientNetB1,\n","                efn.EfficientNetB2, efn.EfficientNetB3, \n","                efn.EfficientNetB4, efn.EfficientNetB5,\n","                efn.EfficientNetB6]\n","        return EFNS[effnet_model]\n","\n","    model_dict = {\n","        \"efficientnet\": get_efficientnet,\n","        \"resnet50\": tf.keras.applications.ResNet50,\n","        \"vgg19\": tf.keras.applications.VGG19,\n","        \"Xception\": tf.keras.applications.Xception,\n","    }   \n","\n","\n","    # For Image Input\n","    img_inp = keras.layers.Input(shape=(dim, dim, 3), name=\"img_input\")\n","    if model_name.find('efficientnet') != -1:\n","        base = model_dict['efficientnet'](model_name[-1])(input_shape=(dim,dim,3),\n","                                                          weights='imagenet',\n","                                                          include_top=False)(img_inp)\n","    else:\n","        base = model_dict[model_name](input_shape=(dim,dim,3),\n","                                      weights='imagenet',\n","                                      include_top=False)(img_inp)\n","    \n","    pooling_layer = keras.layers.GlobalAveragePooling2D()(base)\n","\n","    if not image_only:\n","        flatten_layer = keras.layers.Flatten()(pooling_layer)\n","        # For metadata input\n","        metadata_inp = keras.layers.Input(shape=(metadata_length), name=\"metadata_input\")\n","\n","        # Concating the pooled features and metadata\n","        concat = keras.layers.Concatenate()([metadata_inp, flatten_layer])\n","\n","        # A dense layer which will try to find a relation between image features and metadata\n","        feature_layer = keras.layers.Dense(feature_vec_size, activation=\"selu\", name=\"featvec\")(concat)\n","\n","        # Normalizing the features\n","        normalized_feature = keras.layers.BatchNormalization(name=\"norm_featvec\")(feature_layer)\n","\n","        # Output\n","        output = keras.layers.Dense(1, activation=\"sigmoid\", name=\"output\")(normalized_feature)\n","    else:\n","        feature_layer = keras.layers.Flatten()(pooling_layer)\n","        normalized_feature = keras.layers.BatchNormalization(name=\"norm_featvec\")(feature_layer)\n","        output = tf.keras.layers.Dense(1,activation='sigmoid')(pooling_layer)\n","    \n","    if normalize_features:\n","        feat_output = normalized_feature\n","    else:\n","        feat_output = feature_layer\n","            \n","    if image_only:\n","        if return_feature_model:\n","            featext_model = keras.Model(inputs=[img_inp], outputs=[feat_output])\n","        model = keras.Model(inputs=[img_inp], outputs=[output])\n","    else:\n","        if return_feature_model:\n","            featext_model = keras.Model(inputs=[metadata_inp, img_inp], outputs=[feat_output])\n","        model = keras.Model(inputs=[metadata_inp, img_inp], outputs=[output])\n","        \n","    model.compile(loss=tfa.losses.SigmoidFocalCrossEntropy(),\n","                  optimizer=keras.optimizers.Nadam(),\n","                  metrics=['AUC'])\n","\n","    if return_feature_model:\n","        return model, featext_model\n","    else:\n","        return model"],"metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["demo_model = build_model(model_name=\"resnet50\", dim=IMG_SIZES[2], return_feature_model=False)"],"metadata":{"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\n94773248/94765736 [==============================] - 1s 0us/step\n","output_type":"stream"}]},{"cell_type":"code","source":["tf.keras.utils.plot_model(demo_model)"],"metadata":{"trusted":true},"execution_count":16,"outputs":[{"execution_count":16,"output_type":"execute_result","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAhMAAAFgCAYAAAAB/fvWAAAABmJLR0QA/wD/AP+gvaeTAAAgAElEQVR4nO3de1xVdb7/8ffiIvfU8FpKhihm5WW0VFTEQDOnrAmU8obllB2ze50651E/bWoeY50mnNGmMkfN0c4BLJ3RREsykMRRJymzqcAL6qilpghCgvD9/TEPdm73BoGFbMDX8/HYj/K7vvu7Pvu799q8Wfu7NpYxxggAAKB+0rw8XQEAAGjeCBMAAMAWwgQAALCFMAEAAGzx8XQBaHlef/115eTkeLoMAG48+eSTGjJkiKfLQAvDmQk0uJycHG3dutXTZQC4wMqVK3Xw4EFPl4EWiDMTuCQGDx6stLQ0T5cB4DyWZXm6BLRQnJkAAAC2ECYAAIAthAkAAGALYQIAANhCmAAAALYQJgAAgC2ECQAAYAthAgAA2EKYAAAAthAmAACALYQJAABgC2ECAADYQpgAAAC2ECbQJOzYsUOWZWnOnDmeLqVOmmvd9ZWfny/LshQXF+fpUuqtJTwGoKkhTAAtXHZ2tizL0ssvv+zpUhrN5fiYAU/y8XQBgCQNHDhQxhhPl1FnzbVuAGhInJkAAAC2ECbQJLhbe7B+/XpZlqV58+Zp06ZNioqKUlBQkLp27aq5c+c6+s2fP1+RkZHy9/dXr169lJaW5nYfJ0+e1MyZM9WpUycFBARo4MCBWrt2rZYuXSrLsrRy5coGr3vr1q2KiYlRUFCQQkNDlZSUpB9//NFpjPP7Z2RkKCoqSoGBgerQoYMeeOABHTt2zKn/okWLqq23atvq1aslSS+//LKGDx8uSXrhhRdkWZbjVmXYsGGyLEu5ubl1fvzN9THXRVZWliZNmqSIiAj5+fmpffv2uuOOO/TZZ5859cvJyZFlWZo1a5bbcVJTU2VZll577TVHmzFGixcvVlRUlEJCQhQQEKC+ffvqjTfecDnjdf6cZWZmasSIEQoJCdHAgQPr9biAhsTHHGjytm7dqmeeeUbnzp2TJJWUlOi//uu/5Ofnp6NHj+rVV1919P322291zz33qEePHurXr5+j/aefftItt9zi9APzH//4h8aNG6cJEyZckrq3b9+u5557TmfPnnXUvWzZMu3fv1+ZmZku/bds2aKnn35aFRUVkqTS0lItWrRI2dnZ2r59u4KDgy9JnQ2ppT3mo0ePasSIEU5tx48f19q1a7V+/XplZGQoOjpakjRkyBDddNNNWrZsmebOnetS+5/+9CcFBQXp17/+taR/B4kpU6ZoxYoVTv2+/PJLzZo1S1988YUWLlzoUtOWLVucjofKysoGe7xAfXFmAk1eSkqKZs2apf3796u4uFgrV66Ur6+vXnzxRb355ptatGiRfvjhB504cUJPPfWUKisrlZyc7DTG/PnzlZubq8jISGVkZKioqEj79u3TrFmzlJKScknqfu+993TfffcpLy9PJSUlys7OVlhYmLKysvTFF1+49E9LS9OUKVOUl5en4uJiZWVl6cYbb9Q333yjV155pV41PP/889q8ebMk6aWXXpIxxnGrkp2dLWOMU/iqr+bymGvLsiyNGjVKa9as0cGDB1VWVqbvv/9eqamp8vPzczpDJkmPPvqoioqK9Je//MWp/euvv1ZmZqamTp2qNm3aSJKWL1+uFStW6MYbb9S6det04sQJFRcXKzMzU3379tU777yjnJwcl5rS0tI0depUffvttzp37pw+//zzOj8uoKERJtDkjRkzRsnJybrmmmsUFBSk+Ph4jRs3ToWFhZozZ46mT5+u9u3b68orr9Qrr7yi1q1b6+uvv3YaY+XKlbIsS++//75uueUWBQcHq1u3bvrjH/+oUaNGXZK6R48erTfffFMREREKCAjQ0KFD9Z//+Z+S5PYH680336zFixcrIiJCQUFBGj58uFavXi1fX996fQTjCS3tMXfs2FG/+93vtGzZMg0aNEiBgYHq2LGjJkyYoDNnzmjXrl1O/SdMmKBOnTrpT3/6k1N71b8feeQRR9uSJUvk7e2tDRs26LbbbtOVV16poKAgRUdH67333pMk/fWvf3WpafDgwVq0aJF69uwpb2/vhn7IQL0QJtDkXXiaWZKuueYaSXKcYq7i7e2tq6++Wt9//71T+549e3T11Vfr+uuvdxnr1ltvbcBqfxYTE+PSFh4eLkkqKipy2TZ69GiXz/XDw8PVs2dP7dmz55LU2NBa2mPesmWLoqKilJaWpsOHDzs+WqhSWlrq9O9WrVrpoYce0ldffaWsrCxJUnFxsf7yl79o9OjRuu666xx9d+/erYqKCnXt2lU+Pj7y9vaWl5eXvLy8HK/TAwcOuNQUFxdX7/UfwKVCmECT5+/v79JW9WZa3TZ3nyNX9wZ8qS7tDAgIqLYGO/v08vr3YevuMV74w62xtbTHPHfuXJWVlWn27NnKz89XaWmpKisrZYxRZGSk2/s89NBDatWqleNsxF/+8hedPn1ajz32mFO/qsdSUVGhiooKx7jnz1NZWZnL+KGhoQ318IAGQ5jAZaF79+46dOiQy8cfkvTxxx97oCJXH330kcsP3L179+q7775T9+7dHW0dOnSQJO3bt89ljE8++cSlreoH8YW/VTcFTf0x7927Vx07dtScOXPUvXt3+fv7y7Is7dmzR3l5eW7v07FjRyUmJuqDDz7Q0aNH9eabb6pHjx667bbbnPr16tVLgYGBOnXqlNO6jvNvTeGjHqA2CBO4LMTHx8sYo4SEBH366ac6c+aMCgoK9MQTT+ijjz7ydHmSpG3btmn69OnKz8/XmTNnlJ2drV/96lcqLy9XQkKCo1/v3r0lSfPmzdOnn36q0tJS7d+/X0899ZTj8sjzXXnllZKkzZs368SJE43zYGqpqT/msLAw/fDDD1qwYIEKCwtVWFiodevWaezYsTVeRfHYY4+pvLxc999/v3bt2qVHHnnE5czY9OnTVVJSori4OK1du1bHjh1TWVmZCgoK9OGHHyo+Pl4ZGRn1rh1oTIQJXBYeeeQR9enTR//85z81cuRIxwLMP/zhDxo/frwkydfX16M1JiQkaNmyZerRo4eCg4M1fPhwffnll+rVq5eeffZZR7/w8HDdfffdOnr0qEaOHKnAwEBde+21+uMf/6gpU6a4jNujRw9dffXV+uSTT9SuXbtL8j0T9eWpxyxJGRkZTt9Bcf6t6mu4Z8yYIWOMHnnkEbVp00Zt2rTRL3/5SwUFBemGG26o9nENGDBAUVFRSk9P1xVXXKFp06a59ElKStK0adO0Y8cO3XHHHerQoYP8/PzUrVs33X777frggw9UXl5ez5kFGhdhApeFgIAAbdq0STNmzFCHDh3k7++vAQMG6G9/+5vjt962bdt6tMahQ4cqPT1dN998swICAtSuXTtNnz5dWVlZLt9Z8Oc//1n333+/QkND5e/vryFDhmjjxo0uC1Klfy9KXblypYYNG6agoKDGeji10tQf85133qkVK1aoT58+CggIUOfOnTVjxgxlZGTIz8+vxvs+9NBDkqT7779fISEhLtsty9KSJUuUkpKiuLg4tW3bVq1atVJ4eLjuuusurVq1ij9GhmbDMvxhATSwqt/0q/smyqaksrJSAwcOVG5uro4dO+aRxW3r16/XbbfdpuTkZD3++OONvn9PuBwe8xNPPKE//OEPysvLc1r/4UmWZSklJeWSfVEbLltpnJnAZeOpp57S8uXLVVBQoJKSEuXm5mrChAnauXOnYmJiWCWPBlFRUaENGzbozTffVHR0dJMJEsClRJjAZePbb7/VlClT1K1bNwUFBal///56//33FRwcrNdff12SlJubW+3n6Off7rrrLg8/GjRFc+bMkY+Pj8aMGaOzZ886rfsAWjLCBC4bycnJmjZtmtMfbEpISFBOTk6DfJU0UOWqq67S66+/7nI5KNBSsWYCDa45rZkALiesmcAlwpoJAABgD2ECAADYQpgAAAC2ECYAAIAthAkAAGALYQIAANhCmAAAALYQJgAAgC2ECQAAYAthAgAA2EKYAAAAthAmAACALYQJAABgi4+nC0DLtHXrVsdfDwUAtGyECTS4IUOGeLoENJBjx47pn//8p6Kjoz1dChpAQkKCunbt6uky0AJZxhjj6SIANE2pqalKTEwUbxMAapDGmgkAAGALYQIAANhCmAAAALYQJgAAgC2ECQAAYAthAgAA2EKYAAAAthAmAACALYQJAABgC2ECAADYQpgAAAC2ECYAAIAthAkAAGALYQIAANhCmAAAALYQJgAAgC2ECQAAYAthAgAA2EKYAAAAthAmAACALYQJAABgC2ECAADYQpgAAAC2ECYAAIAthAkAAGALYQIAANhCmAAAALYQJgAAgC2ECQAAYAthAgAA2EKYAAAAthAmAACALYQJAABgi4+nCwDQNBw6dEhJSUmqqKhwtB0/flw+Pj6KiYlx6hsZGam33367kSsE0FQRJgBIkrp06aL9+/dr7969LtsyMzOd/j18+PDGKgtAM8DHHAAcpk6dKl9f34v2u+eeexqhGgDNBWECgMOkSZNUXl5eY5/evXvr+uuvb6SKADQHhAkADhEREerTp48sy3K73dfXV0lJSY1cFYCmjjABwMnUqVPl7e3tdtu5c+c0YcKERq4IQFNHmADg5N5771VlZaVLu2VZGjRokLp169b4RQFo0ggTAJxcddVVioqKkpeX89uDt7e3pk6d6qGqADRlhAkALqZMmeLSZoxRfHy8B6oB0NQRJgC4GD9+vNOZCW9vb8XFxalDhw4erApAU0WYAOCibdu2Gj16tGMhpjFGkydP9nBVAJoqwgQAtyZPnuxYiOnj46Nx48Z5uCIATRVhAoBb48aNk5+fn+P/r7jiCg9XBKCp4m9zoFEdOnRIW7Zs8XQZqKVf/OIX2rJli6699lqlpqZ6uhzUEt8FgsZmGWOMp4vA5SM1NVWJiYmeLgNo0XhbRyNL48wEPII3u+ahvLxczz//vF555RVPl4JaIKzDU1gzAaBavr6+mjNnjqfLANDEESYA1CggIMDTJQBo4ggTAADAFsIEAACwhTABAABsIUwAAABbCBMAAMAWwgQAALCFMAEAAGwhTAAAAFsIEwAAwBbCBAAAsIUwAQAAbCFMAE1UcHCwLMtye3vrrbfc3ic7O1ujRo1S69atFRISohEjRuijjz6yXUt+fn61tViWpRtuuMH2PhrLjh07ZFlWk/4DZs2hRuB8hAnAQ7Kzs2VZll5++eUGGW/Dhg2KiYnRxo0bdfr0aRUXFysrK0tjxoxRWlpag+yjuWjouQVQM8IE0IQNHTpUxhiX20MPPeTUr6ysTDNmzFBFRYWefPJJHTt2TCdPntRLL70kY4xmzpyp4uJi2/XExsa6reerr76yPXZjGThwoIwx/NYPNCDCBNACbNy4UQUFBYqOjtbvf/97tWvXTm3atNHzzz+vu+++W8ePH9df//pXT5cJoIUiTKDJW79+vSzL0rx585SZmakRI0YoJCREAwcOdPQxxmjx4sWKiopSSEiIAgIC1LdvX73xxhsyxjiNV1FRofnz52vAgAFq27at2rRpo4EDB+r1119XSUmJ2/1u3bpVMTExCgoKUmhoqJKSkvTjjz+61FrbOl5++WUNHz5ckvTCCy84rT+oj6ysLEnSpEmTXLZNnjxZkpSZmenUPmzYMFmWpdzc3Hrt051FixbJsiytXLmy2m2rV692tNV3jpcuXaro6Gi1adNGISEhuummm/TOO+/o3LlzF53b6tYjlJaW6sUXX9R1110nf39/tW7dWrGxsdqwYYNLDfWpOysrS5MmTVJERIT8/PzUvn173XHHHfrss8/qNMdAk2SARpSSkmLq+rJLT083ksz48eONj4+PkWQkmf79+xtjjKmsrDSTJk1ytF94e+CBB5zGe+aZZ6rtO3/+fJf9Tpw40fj5+bn0jY6Odhq3LnW89NJL1farEhQUZEJDQ0337t2Nr6+v6dy5s0lMTDSff/65yxzFx8cbSeazzz5z2Zafn28kmZEjRzq1Dx061EgyO3fuvOhzkJeXZySZ2NjYGvu98847RpJJS0urdtuqVascbfWZ48TExGrn7uOPP77o3G7fvt1IMrNnz3aMe/bsWcd8XHizLMu8+eabTnXUte4jR45UW5OPj4/JzMx06u+uxtqoz/EFNIBUzkyg2UhLS9PUqVP17bff6ty5c/r8888lScuXL9eKFSt04403at26dTpx4oSKi4uVmZmpvn376p133lFOTo5jnNWrVysoKEjvv/++Tp06pTNnzig3N1dPP/20goODXfb73nvv6b777lNeXp5KSkqUnZ2tsLAwZWVl6YsvvnD0q0sdzz//vDZv3ixJjnUNVbfznThxQnv27FF5ebmOHDmilJQUDRo0SB988IFTv9OnT0uSrrzySpf6q9oKCwud2rOzs2WMUb9+/Wr3BEjKyMhwezVHdnZ2rcdwp7ZzvHjxYqWkpCg0NFRvvfWWDhw4oOLiYm3fvl2//vWv5evrW+u5Pd+CBQv02WefKSwsTGvWrFFhYaEOHDigOXPmyLIsPfHEEzp69Gi967YsS6NGjdKaNWt08OBBlZWV6fvvv1dqaqr8/Pw0d+5cW/MHeJwHkwwuQ3bOTAwePNhUVla6bB85cqTx9vY2hw8fdtm2e/duI8k8++yzTv179OhhysvLa7Xf0aNHu2xbsGCBkWTefffdetexefNmI8m89NJLbvc/btw4s2bNGnPkyBFz+vRps23bNpOQkGAkmbZt25rTp087+sbFxRlJ5p///KfLOCdOnDCSzC9+8YsaH29Nqs5MVHfbvHmzMab+ZyZqO8fDhw93nIGoSU1z6+63/sGDBxtJJicnx6X/gw8+aCSZt99+u951G2PMjh07zPjx481VV13ldIZNkunSpctFa6wNzkzAQ1J9LnlaARpIXFyc2zUFu3fvVkVFhbp27SpJTr+FVv33wIEDjv7JycmKj49XRESEbr31VvXt21dDhgxR//793e43JibGpS08PFySVFRUVO86LubCBZM33XSTUlNTFRsbq02bNmnTpk0aN26cJKl169aS5Paz+pMnTzr1sSM2NlYbN260Pc6FajvH33zzjdq2bau4uLgG3X9+fr5CQ0M1ePBgl2233367Fi5cqPz8fJdtta17y5YtGjlypMrKytzuv7S0tJ6VA00DH3Og2QgNDXXbXllZKenfCysrKipUWVnpclr7/Dfxvn376ptvvtGyZct07bXXavPmzRozZoyuv/567dq1y2X8gIAAl7aqUHP+PupaR31YlqVhw4ZJktNp94iICElye4nml19+6dTnUvLy+vdbStVcnK+mH5i1neNLqT6LX2tb99y5c1VWVqbZs2crPz9fpaWljtdHZGRk/YsGmgjCBJq9Xr16KTAwUKdOnXL7HQjGGJerC3x8fBQdHa3nnntO//u//6t9+/bp9OnTmj59eqPVUfWD99y5c7XehzHGsT6hU6dOjvbo6GhJ0ooVK1zus3z5cqc+l1KHDh0kSfv27XPZ9sknn9gev1evXjp58qQyMjJq7FfXuY2IiNDx48e1bds2l23r1q1z9KmvvXv3qmPHjpozZ466d+8uf39/WZalPXv2KC8vr97jAk0FYQLN3vTp01VSUqK4uDitXbtWx44dU1lZmQoKCvThhx8qPj7e6YdPVFSU3nrrLX399dcqLS1VYWGh1q9frxMnTmjv3r2NVkfVwsjNmzfrxIkTTmO98sorevrpp7Vt2zbHQs4dO3YoMTFRmzZtUuvWrZ1OscfFxTkW/j311FM6fvy4CgsL9fLLL+uDDz5Qu3btdNddd9X7sdVW7969JUnz5s3Tp59+qtLSUu3fv19PPfWU0yWh9ZWUlCRJuvfee/XOO+/o0KFDOnPmjP7xj3/owQcfdFz+WtPcujN+/HhJUmJiotatW6fTp0/r0KFDeumll7Rw4UL5+fk5PlKqj7CwMP3www9asGCBCgsLVVhYqHXr1mns2LFuz+IAzU4jLc4AjDH2FmAmJye73V5ZWWmmTZtW4wLB9PR0R393l/JV3R599NFa7bdq2/mXkta1jnPnzpmrr77a7eWLzz77bI2XEqakpLitydvb2+2lje76X4pLQ40x5u6773Zb85QpU6pdgFnbOa6oqHAsQnV3q1qYWdPcVndpaFRUVLXjVndpaG3rXr16tdtx+/fvb2644QYTGhrqNAYLMNHMcGkomj/LsrRkyRKlpKQoLi5Obdu2VatWrRQeHq677rpLq1atclqw9/e//10PP/ywevfurYCAALVr105Dhw7VokWLlJyc3Gh1eHt7a+XKlRo2bJiCgoKcxvrv//5vLViwQMOHD1e7du3k6+ursLAwTZ48Wdu2bdOECRNc9j9mzBh9+umnio2NVUhIiIKCgjR8+HCtX7/ebf9L5c9//rPuv/9+hYaGyt/fX0OGDNHGjRsb5GMWLy8vpaamauHChRo8eLCCgoJ0xRVX6Oabb9aiRYscZ2tqmlt3WrVqpY0bN2r27NmKjIxUq1atFBISopEjRyo9Pd3l68vr6s4779SKFSvUp08fBQQEqHPnzpoxY4YyMjLk5+dna2ygKbCMaaTVTYCk1NRUJSYmNtqiOuBywvEFD0njzAQAALCFMAEAAGwhTAAAAFsIEwAAwBbCBAAAsIUwAQAAbCFMAAAAWwgTAADAFsIEAACwhTABAABsIUwAAABbCBMAAMAWwgQAALCFMAEAAGwhTAAAAFsIEwAAwBbCBAAAsMXH0wXg8pSamurpEoAWJycnx9Ml4DJFmIBHJCYmeroEAEADsYwxxtNFAGiaUlNTlZiYKN4mANQgjTUTAADAFsIEAACwhTABAABsIUwAAABbCBMAAMAWwgQAALCFMAEAAGwhTAAAAFsIEwAAwBbCBAAAsIUwAQAAbCFMAAAAWwgTAADAFsIEAACwhTABAABsIUwAAABbCBMAAMAWwgQAALCFMAEAAGwhTAAAAFsIEwAAwBbCBAAAsIUwAQAAbCFMAAAAWwgTAADAFsIEAACwhTABAABsIUwAAABbCBMAAMAWwgQAALCFMAEAAGwhTAAAAFsIEwAAwBYfTxcAoGk4duyYVq1a5dS2Y8cOSdLChQud2oODgzVx4sRGqw1A02YZY4yniwDgeWfPnlX79u115swZeXt7S5KMMTLGyMvr55OY5eXlmjp1qt59911PlQqgaUnjYw4AkiQ/Pz+NHz9ePj4+Ki8vV3l5uc6dO6eKigrHv8vLyyWJsxIAnBAmADhMnDhRZWVlNfZp06aNYmNjG6kiAM0BYQKAw8iRI9W+fftqt/v6+mry5Mny8WG5FYCfESYAOHh5eWnixIlq1aqV2+3l5eW69957G7kqAE0dYQKAk3vvvbfajzo6d+6sIUOGNHJFAJo6wgQAJ4MGDdI111zj0u7r66ukpCRZluWBqgA0ZYQJAC6mTJkiX19fpzY+4gBQHcIEABeTJk1yXAZaJSIiQn369PFQRQCaMsIEABe9evVS7969HR9p+Pr66r777vNwVQCaKsIEALemTp3q+CbM8vJyTZgwwcMVAWiqCBMA3LrnnntUUVEhSRowYIAiIiI8XBGApoowAcCta665RjfddJOkf5+lAIDquPyhr9TUVCUmJnqqHgAA0IS5+fugadV+J25KSsqlrQZAk3f69Gn96U9/0nPPPefpUgB4WE5OjubNm+d2W7VhgsVWACRpxIgR6tGjh6fLANAEVBcmWDMBoEYECQAXQ5gAAAC2ECYAAIAthAkAAGALYQIAANhCmAAAALYQJgAAgC2ECQAAYAthAgAA2EKYAAAAthAmAACALYQJAABgC2ECAADY4pEwsWPHDlmWpTlz5tR7jPXr18uyrGr/gpldDVEjLh/uXi+eeA2dOXNGK1as0Lhx49StWzf5+fnpqquuUmJionbu3FmnsZYuXSrLsrRy5coGqS0zM1OTJ09WeHi4AgMDdcUVVygyMlKjR4/WH//4Rx0+fNipf0t5nxgxYoQsy9LIkSMvSQ0tSX5+vizLcrr5+fkpPDxcM2bM0KFDhxqljuZ6PLubv5CQEIWFhWns2LF67bXXdOzYsUtSK2cmgBbkd7/7nSZPnqw1a9aooKBAZWVlOnLkiFJTUzVo0CCtX7++0Ws6e/askpKSFBMToxUrVmjfvn0qLS1VUVGRvvvuO3388cd67LHHNG7cuEav7VLbu3evNm/eLOnfYWrv3r0erqj5KSsr0759+7Rw4UINGDCg0QJFU9AQx3NxcbEOHjyo9PR0PfPMM+revbtWrFjR4LUSJoBLZODAgTLGNOpvMiEhIY43n71796qkpEQ7d+7UqFGjVF5erocffrjRaqkyc+ZMLVu2TEFBQZo9e7a++OILFRUV6cyZM8rLy1NGRoYef/xxdenSpdFru9SWLl0qY4xmzZolY4zeffddT5fULMTGxsoYI2OMioqKlJGRoR49euiHH37Qa6+95pGamtPxfP78/fTTTyooKFBaWppiYmJUVFSkqVOn6sMPP2zQWn0adDQAHvXss8+6tPXr10+rV69Wly5dtHfvXp04cUKhoaGNUs/27du1ePFiBQUFKTs7W/369XPaHhERoYiICN1yyy2NUk9jMsZo2bJl6tChg/7nf/5HqampWrZsmebMmSPLsjxdXrMRHBysW265RX/4wx80duxY7dixw9MlNZqGOJ79/PwUFhamsLAwJSQk6IknntC8efP05JNP6rbbbpOXV8OcU2iwMxMnT57UzJkz1alTJwUEBGjgwIFau3ZtnT53LS0t1YsvvqjrrrtO/v7+at26tWJjY7Vhw4Ya75eRkaGoqCgFBgaqQ4cOeuCBB9x+LpSVlaVJkyYpIiJCfn5+at++ve644w599tln9X7c1antvjIzM2VZlh577DG343zwwQeyLEu/+93vHG3GGC1evFhRUVEKCQlRQECA+vbtqzfeeEPGGKf7n/+ZcWZmpkaMGKGQkBANHDiwzrVWqetzXZd6a+v8x1Xb56je9mwAABXtSURBVL+ur6/6vh6ruPuM9fy6t27dqpiYGAUFBSk0NFRJSUn68ccfXcZpiGMrMDBQYWFh8vHxUVBQkNO2U6dOadasWercubPT+NUZNmyYLMtSbm7uRfe7ZMkSSdLjjz/uEiTqq7m8T3zyyScqKCjQPffcI39/fyUmJmr//v3atGmTow/Hf/2Of4njubrj+WLmzp2rTp066bvvvqvVMVxr5gIpKSnGTXONSktLTb9+/Ywkp5tlWSYxMdFIMmlpaY7+27dvN5LM7NmzHW1nz541Q4cOdRmjapw333zTaZ/p6elGkhk/frzx9vZ2uU+vXr1MUVGRo/+RI0fcji3J+Pj4mMzMTKfx3dVYW3XdV8+ePc2VV15pfvrpJ5exxo4da7y9vc2//vUvY4wxlZWVZtKkSdWO/8ADD1Q7Tz4+Po5+/fv3r1etdX2u61pvbdX1+a/r66uu/d29Xty1VdU9ceJE4+fn5zJ2dHS0rfmuzjfffGO8vLzM3XffbXv8qnnZuXPnRfdbNfb27dsv2vdCzf19YvLkyUaS2bZtmzHGmL///e9GkpkyZYpTP47/n+Xl5RlJJjY21tFWXFxsNm3aZCIjI40k89hjj3E8V3M8u5s/d5KSkowk8/bbb190X+erIR+kNkiYePXVV40kExkZaTIyMkxRUZHZt2+feeSRRxyTdbEw8fvf/95IMmFhYWbNmjWmsLDQHDhwwMyZM8d4eXkZf39/c+TIEUf/qidRkpk2bZrJy8szxcXFJisry9x4441Gknn++ecd/Y8ePWpGjRpl1qxZYw4ePGjKysrM999/b1JTU01QUJC57bbbnB6TnTBR133NnTvXSDL/93//59R+6NAh4+3tbe644w5H27Jly4wkc+ONN5p169aZEydOmOLiYpOZmWn69u1rJJktW7a4naf777/ffPvtt+bcuXP1rrWuz3Vd662tuj7/dX191bV/Xd98JJmHHnrI5OXlmZKSEpOdnW3CwsKMJJObm1vv+XanuLjY/OIXvzBt27Y1+/btc9p2/vgbN240RUVFZu/evWbmzJm1Hr8mnTp1MpLMsWPHXLaNGDHC5U11165dju3N+X2isLDQBAYGmp49ezq19+zZ0wQGBprTp0872jj+f6636odhdbcOHTqYgwcPcjxXczzXNkz8v//3/4wk89vf/rbGfhe65GHi5ptvNpZlma+++spl26hRo2oVJgYPHmwkmZycHJcxHnzwQZcUVfUk3nzzzaaystKp/549e4yvr6/p1auXU/uOHTvM+PHjzVVXXeWU0iWZLl26OPW1Eybquq+jR48aHx8fM2rUKKf2l19+2Ugyq1evdrSNHDnSeHt7m8OHD7vsc/fu3UaSefbZZx1tVfM0ePBgl3mqT611fa7rWm9t1fX5r+vrq6796/rmM3r0aJdxFyxYYCSZd99919FW1/m+UHFxsYmNjTWBgYEuv2UaY8ygQYOqHT82NrbJhYnm8j6xcOFCI8n85je/cWp/8cUXjSSzaNEiRxvH/8/1ugsTvr6+plu3bubBBx80Bw4cMMZwPFd3PNc2TLzwwgsNHiYaZM3Enj17dPXVV+v666932XbrrbfWaoz8/HyFhoZq8ODBLttuv/12R58LjR492mUxU3h4uHr27Kk9e/Y42rZs2aKoqCilpaXp8OHDOnfunNN9SktLa1VnbdR1Xx07dtTtt9+ujRs3qqCgQJJkjNGSJUvUqVMn/fKXv3T03b17tyoqKtS1a1f5+PjI29tbXl5e8vLycsz/gQMHXGqKi4tzu+irrrXW9bmub721Vdvnv66vr/q+HmsrJibGpS08PFySVFRU5Gizc2ydPHlSo0aN0tatW7Vu3TpFR0e79MnPz692/DFjxlzsYVxUp06dJEn79+932fbpp586VpzfeeedtRqvubxPLF26VJI0adIkp/bJkydL+nkticTx767e869GqLo09O2331bXrl0lcTxXdzzXVtXrrH379vUe40INtgCzutXJpg4LbC7lCue5c+eqrKxMs2fPVn5+vkpLS1VZWSljjCIjIz2+r1//+teONxDp32+0e/bs0bRp0+Tj8/NFN5WVlZKkiooKVVRUOMY9f57Lyspcxq9utW99aq3Lc13fei+Fur6+LuXrMSAgoNr9XTiP9Tm2jhw5ohEjRmjXrl1KT0/XiBEjbFRbf0OGDJEk/e1vf2uwMZv6+8R3332nLVu2SJK6d+/u9AVC3bt3lyR99tlnysvLc9yH47/uOJ7r5+zZs/roo48kSQMGDKj3OBdqkDDRvXt3HTp0SF9//bXLto8//rhWY0REROj48ePatm2by7Z169Y5+lzoo48+cnkS9u7dq++++85x4Fa1dezYUXPmzFH37t3l7+8vy7K0Z88ep4O6IdRnX2PGjFGXLl20ZMkSVVZWatGiRZKk+++/36lfr169FBgYqFOnTjkOygtvdfnGwrrWWtfnuqHrvVBtn/+6vr7q+3psaPU5tvbs2aOhQ4eqoKBAGzZs0PDhw6sdPyIiQv/617+0e/dul20N8QVXSUlJkqTk5GR9+eWXtsdrDu8TVWcl6tKP479uOJ7r77nnntPRo0fVs2fPBrvCSmqgMBEfHy9jjBISEvTpp5/qzJkzKigo0BNPPOFIQBczfvx4SVJiYqLWrVun06dP69ChQ3rppZe0cOFC+fn5uf2GvG3btmn69OnKz8/XmTNnlJ2drV/96lcqLy9XQkKCo19YWJh++OEHLViwQIWFhSosLNS6des0duxYR3puKPXZl7e3t6ZNm6YDBw4oNTVVH3zwgUaMGKEePXo49Zs+fbpKSkoUFxentWvX6tixYyorK1NBQYE+/PBDxcfHKyMj45LVWtfnuqHrvVBtn/+6vr7q+3psaHWd76+++krDhg3TyZMn9fHHHysqKqpW48fHx+uTTz5RcXGx9u3bp4cfftjW81Jl0KBBmjZtmoqLizV06FD95je/0a5du1RSUqKffvpJBw4c0PLlyx1B42K/PTb194nKykotW7ZMkvTFF1+4/eH51VdfSZKWLVvmGJPjv244nmvv7NmzOnjwoN5//33dcsstmjdvnizL0muvvdZg3zEhyXUlRX0WYJaUlJg+ffq4LJyxLMuMHz/eZRFRdZd8RUVFVbuKt7pLvhISEmp1ydfq1avdjtu/f39zww03mNDQUKfx7SzArOu+quzbt89YlmVat25tJJlly5a59KmsrDTTpk2rdp4kmfT0dJd5Sk5ObpBa6/pc17Xe2qrr81/X11dd+9d1wZa756Nq2/z58+s939OnT69xriXnSzov9aWhVfuo6fJAScbb29vMmTPHVFRU1Dh/Tf19Yv369UZyvSTwQjExMUaS2bBhg6ON47/2Cwg5nt0fzxe7GkaSCQ4ONkuXLq1xfqtzyRdgBgQEaNOmTZoxY4Y6dOggf39/DRgwQH/729/Uu3dvSVLbtm1rHKNVq1bauHGjZs+ercjISLVq1UohISEaOXKk0tPT9dBDD7m939ChQ5Wenq6bb75ZAQEBateunaZPn66srCwFBwc7+t15551asWKF+vTpo4CAAHXu3FkzZsxQRkaG/Pz8GmIabO+rW7duiouLU2FhoVq3bu30G1MVy7K0ZMkSpaSkKC4uTm3btlWrVq0UHh6uu+66S6tWrVJcXNwlq7Wuz3VD13uh2j7/dX191ff12NAa4tiqib+/vzZt2qSZM2eqY8eO8vf3V//+/bVq1aoGWYBZtY/ly5crIyNDkyZNUrdu3eTv76/AwEBdd911evTRR/XNN99o9uzZF/1Nqam/T1SteZg1a1aN/R555BGn/hLHf11wPNdeYGCgunTpoltvvVWvvvqq9uzZ4/j4sUHVIXnUWUVFhenfv7+xLMscP368QcZE09TYz/XFfuNq6Ti20JTwerSnuczfJT8zIUlPPfWUli9froKCApWUlCg3N1cTJkzQzp07FRMT02h/CwCXHs9142K+0ZTwerSnpc5fg/2hr2+//Vavv/66S3twcLDb9uYkNzdX/fv3v2i/O++8U6tXr26EijzrUjzXzHH1WvKxheaH16M9LXX+GuzMRHJysqZNm+b0x2ISEhKUk5PToJefwPN4rhsX842mhNejPS11/ixjnC++Tk1NVWJioq2/5gYAAFqWGvJBWgNeZAoAAC5HhAkAAGALYQIAANhCmAAAALYQJgAAgC2ECQAAYAthAgAA2EKYAAAAthAmAACALYQJAABgC2ECAADYQpgAAAC2ECYAAIAtPtVtsCyrMesAAADNlEuYiIqKUkpKiidqAdDE5OTkaN68ebwnAKiRZdz8YXIAkKTU1FQlJiaKtwkANUhjzQQAALCFMAEAAGwhTAAAAFsIEwAAwBbCBAAAsIUwAQAAbCFMAAAAWwgTAADAFsIEAACwhTABAABsIUwAAABbCBMAAMAWwgQAALCFMAEAAGwhTAAAAFsIEwAAwBbCBAAAsIUwAQAAbCFMAAAAWwgTAADAFsIEAACwhTABAABsIUwAAABbCBMAAMAWwgQAALCFMAEAAGwhTAAAAFsIEwAAwBbCBAAAsIUwAQAAbCFMAAAAWwgTAADAFh9PFwCgaSgvL1dxcbFT25kzZyRJJ0+edGq3LEtt2rRptNoANG2ECQCSpBMnTqhLly6qqKhw2XbllVc6/TsmJkabNm1qrNIANHF8zAFAktSpUydFR0fLy6vmtwXLsnTvvfc2UlUAmgPCBACHKVOmyLKsGvt4eXkpPj6+kSoC0BwQJgA4xMfHy9vbu9rt3t7eGjNmjEJDQxuxKgBNHWECgMMVV1yhMWPGyMfH/XIqY4wmT57cyFUBaOoIEwCcTJ482e0iTElq1aqVbr/99kauCEBTR5gA4OSOO+5QYGCgS7uPj49+9atfKTg42ANVAWjKCBMAnPj7++vuu++Wr6+vU/u5c+c0adIkD1UFoCkjTABwMXHiRJWXlzu1XXHFFRo1apSHKgLQlBEmALiIi4tz+qIqX19f3XPPPWrVqpUHqwLQVBEmALjw8fHRPffc4/ioo7y8XBMnTvRwVQCaKsIEALfuvfdex0cdHTt21PDhwz1cEYCmijABwK2hQ4fqqquukvTvb8a82NdsA7h88Ye+cFkZP368p0toVkJCQiRJO3fuZO7qYMiQIXryySc9XQbQaPhVA5eVlStX6tChQ54uo9kICwtTSEiI2rZt6+lSmo2tW7cqJyfH02UAjYozE7jsPPHEE5owYYKny2g2UlNTma864AwOLkecmQBQI4IEgIshTAAAAFsIEwAAwBbCBAAAsIUwAQAAbCFMAAAAWwgTAADAFsIEAACwhTABAABsIUwAAABbCBMAAMAWwgQAALCFMAEAAGwhTAD1tHTpUlmWpZUrV3q6lEaXn58vy7KcbiEhIQoLC9PYsWP12muv6dixY54uE0AjIUwAaBDFxcU6ePCg0tPT9cwzz6h79+5asWKFp8sC0AgIEwDqLTY2VsYYGWP0008/qaCgQGlpaYqJiVFRUZGmTp2qDz/80NNlArjECBMAGoSfn5/CwsKUkJCgTZs26fHHH1dlZaWefPJJVVZWero8AJcQYQK4iFOnTmnWrFnq3LmzAgICNHDgQK1du7ba/sYYLV68WFFRUQoJCVFAQID69u2rN954Q8YYp77r16+XZVmaN2+etm7dqpiYGAUFBSk0NFRJSUn68ccfnfpXVFRo/vz5GjBggNq2bas2bdpo4MCBev3111VSUlLvOiRp2LBhsixLubm5NmbrZ3PnzlWnTp303XffuYzZXOcIQDUMcBmRZFJSUmrdv7S01PTr189IcrpZlmUSExONJJOWluboX1lZaSZNmuTSv+r2wAMPOI2fnp5uJJmJEycaPz8/l/7R0dFO/Z955plqx54/f3696zDGmKFDhxpJZufOnRedl7y8PCPJxMbG1tgvKSnJSDJvv/12i5ij2khISDAJCQn1ui/QTKVyZgKowfz585Wbm6vIyEht3LhRRUVF2rt3r/7jP/5DKSkpLv2XL1+uFStW6MYbb9S6det04sQJFRcXKzMzU3379tU777yjnJwcl/u99957uu+++5SXl6eSkhJlZ2crLCxMWVlZ+uKLLxz9Vq9eraCgIL3//vs6deqUzpw5o9zcXD399NMKDg62VUd2draMMerXr1+Dzd8111wjSTp+/HiLmCMA1fB0nAEak+p4ZmLQoEHGsizz1VdfuWyLjY11OTMxcuRI4+3tbQ4fPuzSf/fu3UaSefbZZx1tVb91jx492qX/ggULjCTz7rvvOo3fo0cPU15eXmPdda2jrmp7ZuKFF14wksxvf/vbetfW3OaIMxO4DKX6eCbCAM1Dfn6+rr76al1//fUu28aMGaOMjAyntt27d6uiokJdu3aVJMeVDlX/L0kHDhxwGSsmJsalLTw8XJJUVFTkaEtOTlZ8fLwiIiJ06623qm/fvhoyZIj69+/fIHU0tIKCAklS+/btbdfWUucIaAn4mANoQFVXLVRUVKiiokKVlZVOP6QkqayszOV+AQEBLm2WZUmS03379u2rb775RsuWLdO1116rzZs3a8yYMbr++uu1a9cu23U0pLNnz+qjjz6SJA0YMMB2bS1xjoCWgjMTQA0iIiK0bds27d692+XsxPr161369+rVS59//rkOHz6s1q1bX5KafHx8FB0drejoaElSSUmJIiMjNX36dG3btq3R6riY5557TkePHlXPnj2d1mEwR0DLw5kJoAbx8fEyxig+Pl6ffPKJiouLtW/fPj388MMuH3FI0vTp01VSUqK4uDitXbtWx44dU1lZmQoKCvThhx8qPj7e7f1qKyoqSm+99Za+/vprlZaWqrCwUOvXr9eJEye0d+/eRqvDnbNnz+rgwYN6//33dcstt2jevHmyLEuvvfaavLx+fqu5nOcIaLEacYEG4HFqhEtDp02bVu3lhpJMenq6o3/V4sLk5GSXfVdtO/9yRneXRlbdHn300XrXYUz9Lg2t6RYcHGyWLl3qct/mPEe1wQJMXIa4NBSoib+/vzZt2qSZM2eqY8eO8vf3V//+/bVq1SqNGTPGpb9lWVqyZIlSUlIUFxentm3bqlWrVgoPD9ddd92lVatWKS4urt71/P3vf9fDDz+s3r17KyAgQO3atdPQoUO1aNEiJScnN1od7gQGBqpLly669dZb9eqrr2rPnj1KSkpy6Xc5zxHQUlnG8DVvuHxYlqWUlBRNmDDB06WghRo/frwkKS0tzcOVAI0mjTMTAADAFsIEAACwhTABAABsIUwAAABbCBMAAMAWwgQAALCFMAEAAGwhTAAAAFsIEwAAwBbCBAAAsIUwAQAAbCFMAAAAWwgTAADAFsIEAACwhTABAABsIUwAAABbCBMAAMAWH08XADS25ORkpaWleboMtFBbt27V4MGDPV0G0KgIE7isJCQkeLoEtHCDBw/WkCFDPF0G0KgsY4zxdBEAAKDZSmPNBAAAsIUwAQAAbCFMAAAAWwgTAADAlv8PWdoOliS5pBcAAAAASUVORK5CYII=\n","text/plain":"<IPython.core.display.Image object>"},"metadata":{}}]},{"cell_type":"markdown","source":["# Step 4: Train Schedule\n","This is a common train schedule for transfer learning. The learning rate starts near zero, then increases to a maximum, then decays over time. Consider changing the schedule and/or learning rates. Note how the learning rate max is larger with larger batches sizes. This is a good practice to follow."],"metadata":{}},{"cell_type":"code","source":["def get_lr_callback(batch_size=8):\n","    lr_start   = 0.000005\n","    lr_max     = 0.00000125 * REPLICAS * batch_size\n","    lr_min     = 0.000001\n","    lr_ramp_ep = 5\n","    lr_sus_ep  = 0\n","    lr_decay   = 0.8\n","   \n","    def lrfn(epoch):\n","        if epoch < lr_ramp_ep:\n","            lr = (lr_max - lr_start) / lr_ramp_ep * epoch + lr_start\n","            \n","        elif epoch < lr_ramp_ep + lr_sus_ep:\n","            lr = lr_max\n","            \n","        else:\n","            lr = (lr_max - lr_min) * lr_decay**(epoch - lr_ramp_ep - lr_sus_ep) + lr_min\n","            \n","        return lr\n","\n","    lr_callback = tf.keras.callbacks.LearningRateScheduler(lrfn, verbose=False)\n","    return lr_callback"],"metadata":{"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"markdown","source":["## Train Model\n","Our model will be trained for the number of FOLDS and EPOCHS you chose in the configuration above. Each fold the model with lowest validation loss will be saved and used to predict OOF and test. Adjust the variables `VERBOSE` and `DISPLOY_PLOT` below to determine what output you want displayed. The variable `VERBOSE=1 or 2` will display the training and validation loss and auc for each epoch as text. The variable `DISPLAY_PLOT` shows this information as a plot. "],"metadata":{}},{"cell_type":"code","source":["import os\n","\n","root_checkpointdir = os.path.join(os.curdir, \"my_checkpoints\")\n","def get_model_checkpoint_path(model_name, fold, img_size):\n","    file_name = model_name+\"_\"+\"fold\"+str(fold) + \"_\" + str(img_size)\n","    path = os.path.join(root_checkpointdir, model_name)\n","    os.makedirs(path, exist_ok=True)\n","    return os.path.join(path, file_name)"],"metadata":{"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":["# USE VERBOSE=0 for silent, VERBOSE=1 for interactive, VERBOSE=2 for commit\n","VERBOSE = 2\n","DISPLAY_PLOT = True\n","\n","def train_model(model_name=None, EPOCHS=EPOCHS, FOLDS=FOLDS, IMG_SIZES=IMG_SIZES,\n","                BATCH_SIZES=BATCH_SIZES, REPLICAS=REPLICAS, SEED=SEED, DATA_PATH=DATA_PATH,\n","                DATA_PATH2=DATA_PATH2, TTA=TTA, VERBOSE=VERBOSE, DISPLAY_PLOT=DISPLAY_PLOT):\n","                #files_train=files_train, files_test=files_test):\n","    \n","    skf = KFold(n_splits=FOLDS,shuffle=True,random_state=SEED)\n","    oof_pred = []; oof_tar = []; oof_val = []; oof_names = []; oof_folds = [] \n","    best_auc = 0\n","    best_fold = None\n","    best_model_history = None\n","    #preds = np.zeros((count_data_items(files_test),1))\n","\n","    for fold,(idxT,idxV) in enumerate(skf.split(np.arange(15))):\n","        if fold != 2:\n","            continue\n","        # DISPLAY FOLD INFO\n","        print('#'*25); print('#### FOLD',fold+1)\n","        print('#### Image Size %i with %s and batch_size %i'%\n","              (IMG_SIZES[fold], model_name,BATCH_SIZES[fold]*REPLICAS))\n","\n","        # CREATE TRAIN AND VALIDATION SUBSETS\n","        files_train = tf.io.gfile.glob([DATA_PATH[fold] + '/train%.2i*.tfrec'%x for x in idxT])\n","        if INC2019[fold]:\n","            files_train += tf.io.gfile.glob([DATA_PATH2[fold] + '/train%.2i*.tfrec'%x for x in idxT*2+1])\n","            print('#### Using 2019 external data')\n","        if INC2018[fold]:\n","            files_train += tf.io.gfile.glob([DATA_PATH2[fold] + '/train%.2i*.tfrec'%x for x in idxT*2])\n","            print('#### Using 2018+2017 external data')\n","        np.random.shuffle(files_train); print('#'*25)\n","        files_valid = tf.io.gfile.glob([DATA_PATH[fold] + '/train%.2i*.tfrec'%x for x in idxV])\n","        files_test = np.sort(np.array(tf.io.gfile.glob(DATA_PATH[fold] + '/test*.tfrec')))\n","\n","        # BUILD MODEL\n","        K.clear_session()\n","        with strategy.scope():\n","            model, feat_model = build_model(model_name=model_name, dim=IMG_SIZES[fold],\n","                                            return_feature_model=True, image_only=IMG_ONLY)\n","\n","        # SAVE BEST MODEL EACH FOLD\n","        checkpoint_file_path = get_model_checkpoint_path(model_name, fold, IMG_SIZES[fold])\n","        sv = tf.keras.callbacks.ModelCheckpoint(\n","            f'{checkpoint_file_path}.h5', monitor='val_loss', verbose=0, save_best_only=True,\n","            save_weights_only=True, mode='min', save_freq='epoch')\n","\n","        # Getting DATASETS:\n","        train_dataset = get_dataset(files_train, augment=True, shuffle=True, repeat=True,\n","                                    dim=IMG_SIZES[fold],batch_size = BATCH_SIZES[fold],\n","                                    image_only=IMG_ONLY)\n","        \n","        valid_dataset = get_dataset(files_valid, augment=False, shuffle=False,\n","                                    repeat=False,dim=IMG_SIZES[fold],\n","                                    image_only=IMG_ONLY)\n","        \"\"\"for item in valid_dataset.take(1):\n","            print(item)\n","        print(asdasd)\"\"\"\n","        # TRAIN\n","        print('Training...')\n","        history = model.fit(\n","                train_dataset,\n","                epochs=EPOCHS[fold],\n","                validation_data=valid_dataset, \n","                callbacks = [sv,\n","                             get_lr_callback(BATCH_SIZES[fold])], \n","                steps_per_epoch=count_data_items(files_train)/BATCH_SIZES[fold]//REPLICAS,\n","                #class_weight = {0:1,1:2},\n","                verbose=VERBOSE,\n","        )\n","\n","        print('Loading best model...')\n","        model.load_weights(f'{checkpoint_file_path}.h5')\n","        head_tail = os.path.split(f'{checkpoint_file_path}.h5')\n","        feat_model.save_weights(f'{os.path.join(head_tail[0], f\"feat_{model_name}\")}.h5')\n","        # PREDICT OOF USING TTA\n","        print('Predicting OOF with TTA...')\n","        valid_dataset = get_dataset(files_valid,labeled=False,return_image_names=False,augment=True,\n","                                    repeat=True,shuffle=False,dim=IMG_SIZES[fold],batch_size=BATCH_SIZES[fold]*4,\n","                                    image_only=IMG_ONLY)\n","        ct_valid = count_data_items(files_valid)\n","        STEPS = TTA * ct_valid/BATCH_SIZES[fold]/4/REPLICAS\n","        pred = model.predict(valid_dataset,steps=STEPS,verbose=VERBOSE)[:TTA*ct_valid,] \n","        oof_pred.append(np.mean(pred.reshape((ct_valid,TTA),order='F'),axis=1) )                 \n","        #oof_pred.append(model.predict(get_dataset(files_valid,dim=IMG_SIZES[fold]),verbose=1))\n","\n","        # GET OOF TARGETS AND NAMES\n","        valid_dataset = get_dataset(files_valid, augment=False, repeat=False, dim=IMG_SIZES[fold],\n","                                     labeled=True, return_image_names=True, image_only=IMG_ONLY)\n","        oof_tar.append(np.array([target.numpy() for img, target in iter(valid_dataset.unbatch())]))\n","        oof_folds.append(np.ones_like(oof_tar[-1],dtype='int8')*fold)\n","        valid_dataset = get_dataset(files_valid, augment=False, repeat=False, dim=IMG_SIZES[fold],\n","                                    labeled=False, return_image_names=True, image_only=IMG_ONLY)\n","        oof_names.append(np.array([feature_dict[1].numpy().decode(\"utf-8\") for feature_dict in iter(valid_dataset.unbatch())]))\n","\n","        # PREDICT TEST USING TTA\n","        print('Predicting Test with TTA...')\n","        test_dataset = get_dataset(files_test,labeled=False,return_image_names=False,augment=True,\n","                                   repeat=True,shuffle=False,dim=IMG_SIZES[fold],batch_size=BATCH_SIZES[fold]*4,\n","                                   image_only=IMG_ONLY)\n","        ct_test = count_data_items(files_test)\n","        STEPS = TTA * ct_test/BATCH_SIZES[fold]/4/REPLICAS\n","        pred = model.predict(test_dataset,steps=STEPS,verbose=VERBOSE)[:TTA*ct_test,] \n","        #preds[:,0] += np.mean(pred.reshape((ct_test,TTA),order='F'),axis=1) * WGTS[fold]\n","\n","        # REPORT RESULTS\n","        auc = roc_auc_score(oof_tar[-1],oof_pred[-1])\n","        if auc > best_auc:\n","            best_fold = fold\n","            best_model_history = history\n","        oof_val.append(np.max( history.history['val_auc'] ))\n","        print('#### FOLD %i OOF AUC without TTA = %.3f, with TTA = %.3f'%(fold+1,oof_val[-1],auc))\n","\n","        # PLOT TRAINING\n","        if DISPLAY_PLOT:\n","            plt.figure(figsize=(15,5))\n","            plt.plot(np.arange(EPOCHS[fold]),history.history['auc'],'-o',label='Train AUC',color='#ff7f0e')\n","            plt.plot(np.arange(EPOCHS[fold]),history.history['val_auc'],'-o',label='Val AUC',color='#1f77b4')\n","            x = np.argmax( history.history['val_auc'] ); y = np.max( history.history['val_auc'] )\n","            xdist = plt.xlim()[1] - plt.xlim()[0]; ydist = plt.ylim()[1] - plt.ylim()[0]\n","            plt.scatter(x,y,s=200,color='#1f77b4'); plt.text(x-0.03*xdist,y-0.13*ydist,'max auc\\n%.2f'%y,size=14)\n","            plt.ylabel('AUC',size=14); plt.xlabel('Epoch',size=14)\n","            plt.legend(loc=2)\n","            plt2 = plt.gca().twinx()\n","            plt2.plot(np.arange(EPOCHS[fold]),history.history['loss'],'-o',label='Train Loss',color='#2ca02c')\n","            plt2.plot(np.arange(EPOCHS[fold]),history.history['val_loss'],'-o',label='Val Loss',color='#d62728')\n","            x = np.argmin( history.history['val_loss'] ); y = np.min( history.history['val_loss'] )\n","            ydist = plt.ylim()[1] - plt.ylim()[0]\n","            plt.scatter(x,y,s=200,color='#d62728'); plt.text(x-0.03*xdist,y+0.05*ydist,'min loss',size=14)\n","            plt.ylabel('Loss',size=14)\n","            plt.title('FOLD %i - Image Size %i, %s, inc2019=%i, inc2018=%i'%\n","                    (fold+1,IMG_SIZES[fold], model_name,INC2019[fold],INC2018[fold]),size=18)\n","            plt.legend(loc=3)\n","            plt.show()  \n","    \n","    root_checkpoint_dir = \"my_checkpoints\"\n","    model = build_model(model_name=model_name, dim=IMG_SIZES[best_fold])\n","    model.load_weights(os.path.join(root_checkpoint_dir, model_name,\n","                                    f'{model_name}_fold{best_fold}_{IMG_SIZES[best_fold]}.h5'))\n","    \n","    return model, best_model_history"],"metadata":{"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":["def get_dataset(files, image_only=True, augment = False, shuffle = False, repeat = False, \n","                labeled=True, return_image_names=False, batch_size=16, dim=256):\n","    \n","    read_labeled_unlabeled = partial(read_tfrecord, image_only=image_only,\n","                                     labeled=labeled, return_image_names=return_image_names)\n","    #read_unlabeled_tfrecord = partial(read_tfrecord, image_only=image_only, labeled=False)\n","    \n","    ds = tf.data.TFRecordDataset(files, num_parallel_reads=AUTO)\n","    ds = ds.cache()\n","    \n","    if repeat:\n","        ds = ds.repeat()\n","    \n","    if shuffle: \n","        ds = ds.shuffle(1024*8)\n","        opt = tf.data.Options()\n","        opt.experimental_deterministic = False\n","        ds = ds.with_options(opt)\n","    \n","    ds = ds.map(read_labeled_unlabeled, num_parallel_calls=AUTO)\n","    ds = ds.map(lambda features, target_or_image_names: (prepare_image(features, augment=augment, image_only=image_only,\n","                                                            dim=dim),\n","                                                         target_or_image_names),\n","                num_parallel_calls=AUTO)\n","    \n","    if labeled:\n","        ds = ds.map(lambda features, target: (features, tf.cast(target, tf.float32)),\n","                    num_parallel_calls=AUTO)\n","     \n","    ds = ds.batch(batch_size * REPLICAS)\n","    ds = ds.prefetch(AUTO)\n","    return ds"],"metadata":{"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":["models_history = dict()\n","models = dict()"],"metadata":{"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":["model_names_list = ['efficientnet_b3', 'efficientnet_b6', 'efficientnet_b7', 'resnet50', 'Xception', 'vgg19']\n","for model_name in model_names_list:\n","    print(\"\\n---------------------- TRAINING MODEL {model_name} ----------------------\".format(model_name=model_name))\n","    models[model_name], models_history[model_name] = train_model(model_name)"],"metadata":{"trusted":true},"execution_count":22,"outputs":[{"name":"stdout","text":"\n---------------------- TRAINING MODEL efficientnet_b6 ----------------------\n#########################\n#### FOLD 3\n#### Image Size 384 with efficientnet_b6 and batch_size 256\n#### Using 2018+2017 external data\n#########################\nDownloading data from https://github.com/Callidior/keras-applications/releases/download/efficientnet/efficientnet-b6_weights_tf_dim_ordering_tf_kernels_autoaugment_notop.h5\n165527552/165527152 [==============================] - 2s 0us/step\nTraining...\nEpoch 1/15\n118/118 - 336s - loss: 0.0946 - auc: 0.5371 - val_loss: 0.0901 - val_auc: 0.5723\nEpoch 2/15\n118/118 - 100s - loss: 0.0179 - auc: 0.7744 - val_loss: 0.0077 - val_auc: 0.8023\nEpoch 3/15\n118/118 - 100s - loss: 0.0140 - auc: 0.8715 - val_loss: 0.0068 - val_auc: 0.8701\nEpoch 4/15\n118/118 - 100s - loss: 0.0129 - auc: 0.9008 - val_loss: 0.0066 - val_auc: 0.8826\nEpoch 5/15\n118/118 - 99s - loss: 0.0120 - auc: 0.9163 - val_loss: 0.0067 - val_auc: 0.8943\nEpoch 6/15\n118/118 - 99s - loss: 0.0117 - auc: 0.9238 - val_loss: 0.0064 - val_auc: 0.9128\nEpoch 7/15\n118/118 - 99s - loss: 0.0107 - auc: 0.9399 - val_loss: 0.0061 - val_auc: 0.9142\nEpoch 8/15\n118/118 - 100s - loss: 0.0100 - auc: 0.9513 - val_loss: 0.0064 - val_auc: 0.9039\nEpoch 9/15\n118/118 - 99s - loss: 0.0093 - auc: 0.9585 - val_loss: 0.0058 - val_auc: 0.9221\nEpoch 10/15\n118/118 - 99s - loss: 0.0084 - auc: 0.9679 - val_loss: 0.0059 - val_auc: 0.9251\nEpoch 11/15\n118/118 - 99s - loss: 0.0077 - auc: 0.9737 - val_loss: 0.0057 - val_auc: 0.9299\nEpoch 12/15\n118/118 - 99s - loss: 0.0073 - auc: 0.9780 - val_loss: 0.0063 - val_auc: 0.9292\nEpoch 13/15\n118/118 - 99s - loss: 0.0065 - auc: 0.9814 - val_loss: 0.0063 - val_auc: 0.9271\nEpoch 14/15\n118/118 - 99s - loss: 0.0066 - auc: 0.9827 - val_loss: 0.0063 - val_auc: 0.9291\nEpoch 15/15\n118/118 - 99s - loss: 0.0058 - auc: 0.9867 - val_loss: 0.0062 - val_auc: 0.9290\nLoading best model...\nPredicting OOF with TTA...\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-22-65cf2cc34d7a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mmodel_name\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodel_names_list\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n---------------------- TRAINING MODEL {model_name} ----------------------\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mmodels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodels_history\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-19-9ebdcf249967>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model_name, EPOCHS, FOLDS, IMG_SIZES, BATCH_SIZES, REPLICAS, SEED, DATA_PATH, DATA_PATH2, TTA, VERBOSE, DISPLAY_PLOT)\u001b[0m\n\u001b[1;32m     79\u001b[0m         valid_dataset = get_dataset(files_valid,labeled=False,return_image_names=False,augment=True,\n\u001b[1;32m     80\u001b[0m                                     \u001b[0mrepeat\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mIMG_SIZES\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfold\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mBATCH_SIZES\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfold\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m                                     image_only=IMG_ONLY)\n\u001b[0m\u001b[1;32m     82\u001b[0m         \u001b[0mct_valid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcount_data_items\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiles_valid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[0mSTEPS\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTTA\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mct_valid\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mBATCH_SIZES\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfold\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mREPLICAS\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-20-f046583d001f>\u001b[0m in \u001b[0;36mget_dataset\u001b[0;34m(files, image_only, augment, shuffle, repeat, labeled, return_image_names, batch_size, dim)\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_options\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m     \u001b[0mds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mread_labeled_unlabeled\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_parallel_calls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mAUTO\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m     ds = ds.map(lambda features, target_or_image_names: (prepare_image(features, augment=augment, image_only=image_only,\n\u001b[1;32m     22\u001b[0m                                                             dim=dim),\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36mmap\u001b[0;34m(self, map_func, num_parallel_calls, deterministic)\u001b[0m\n\u001b[1;32m   1810\u001b[0m           \u001b[0mnum_parallel_calls\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1811\u001b[0m           \u001b[0mdeterministic\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1812\u001b[0;31m           preserve_cardinality=True)\n\u001b[0m\u001b[1;32m   1813\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1814\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mflat_map\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, input_dataset, map_func, num_parallel_calls, deterministic, use_inter_op_parallelism, preserve_cardinality, use_legacy_function)\u001b[0m\n\u001b[1;32m   4244\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transformation_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4245\u001b[0m         \u001b[0mdataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_dataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4246\u001b[0;31m         use_legacy_function=use_legacy_function)\n\u001b[0m\u001b[1;32m   4247\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdeterministic\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4248\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_deterministic\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"default\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, func, transformation_name, dataset, input_classes, input_shapes, input_types, input_structure, add_to_graph, use_legacy_function, defun_kwargs)\u001b[0m\n\u001b[1;32m   3523\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mtracking\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresource_tracker_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_tracker\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3524\u001b[0m         \u001b[0;31m# TODO(b/141462134): Switch to using garbage collection.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3525\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwrapper_fn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_concrete_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3526\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0madd_to_graph\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3527\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_to_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_default_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mget_concrete_function\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3050\u001b[0m     \"\"\"\n\u001b[1;32m   3051\u001b[0m     graph_function = self._get_concrete_function_garbage_collected(\n\u001b[0;32m-> 3052\u001b[0;31m         *args, **kwargs)\n\u001b[0m\u001b[1;32m   3053\u001b[0m     \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_garbage_collector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelease\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3054\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_get_concrete_function_garbage_collected\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3017\u001b[0m       \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3018\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3019\u001b[0;31m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3020\u001b[0m       \u001b[0mseen_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3021\u001b[0m       captured = object_identity.ObjectIdentitySet(\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   3359\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3360\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmissed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcall_context_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3361\u001b[0;31m           \u001b[0mgraph_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_graph_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3362\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprimary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3363\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[0;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m   3204\u001b[0m             \u001b[0marg_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0marg_names\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3205\u001b[0m             \u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3206\u001b[0;31m             capture_by_value=self._capture_by_value),\n\u001b[0m\u001b[1;32m   3207\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_attributes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3208\u001b[0m         \u001b[0mfunction_spec\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_spec\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[0;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m    988\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moriginal_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munwrap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpython_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    989\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 990\u001b[0;31m       \u001b[0mfunc_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    991\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    992\u001b[0m       \u001b[0;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36mwrapper_fn\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m   3516\u001b[0m           attributes=defun_kwargs)\n\u001b[1;32m   3517\u001b[0m       \u001b[0;32mdef\u001b[0m \u001b[0mwrapper_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=missing-docstring\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3518\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_wrapper_helper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3519\u001b[0m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstructure\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_tensor_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output_structure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3520\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36m_wrapper_helper\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m   3451\u001b[0m         \u001b[0mnested_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnested_args\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3452\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3453\u001b[0;31m       \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mautograph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtf_convert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mag_ctx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mnested_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3454\u001b[0m       \u001b[0;31m# If `func` returns a list of tensors, `nest.flatten()` and\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3455\u001b[0m       \u001b[0;31m# `ops.convert_to_tensor()` would conspire to attempt to stack\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/autograph/impl/api.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    668\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    669\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'ag_error_metadata'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 670\u001b[0;31m           \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    671\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    672\u001b[0m           \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: in user code:\n\n    <ipython-input-12-3211960beff1>:37 read_tfrecord  *\n        return ({\"img_input\": example['image']}, example['image_name'] if return_image_name else 0)\n\n    NameError: name 'return_image_name' is not defined\n"],"ename":"NameError","evalue":"in user code:\n\n    <ipython-input-12-3211960beff1>:37 read_tfrecord  *\n        return ({\"img_input\": example['image']}, example['image_name'] if return_image_name else 0)\n\n    NameError: name 'return_image_name' is not defined\n","output_type":"error"}]},{"cell_type":"code","source":["root_path = \"./my_checkpoints\"\n","for model_name in model_names_list:\n","    model_path = os.path.join(root_path, model_name)\n","    np.save(os.path.join(model_path, f'{model_name}_best_history.npy'),models_history[model_name].history)\n","    models[model_name].save_weights(os.path.join(model_path, f'{model_name}_best_weights.h5'))\n","    \n","#history1=np.load('history1.npy',allow_pickle='TRUE').item()"],"metadata":{"trusted":true},"execution_count":23,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)","\u001b[0;32m<ipython-input-23-78b8ce9eb8a5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mmodel_name\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodel_names_list\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mmodel_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf'{model_name}_best_history.npy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmodels_history\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mmodels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf'{model_name}_best_weights.h5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyError\u001b[0m: 'efficientnet_b6'"],"ename":"KeyError","evalue":"'efficientnet_b6'","output_type":"error"}]},{"cell_type":"markdown","source":["## Calculate OOF AUC\n","The OOF (out of fold) predictions are saved to disk. If you wish to ensemble multiple models, use the OOF to determine what are the best weights to blend your models with. Choose weights that maximize OOF CV score when used to blend OOF. Then use those same weights to blend your test predictions."],"metadata":{}},{"cell_type":"code","source":["\"\"\"# COMPUTE OVERALL OOF AUC\n","oof = np.concatenate(oof_pred); true = np.concatenate(oof_tar);\n","names = np.concatenate(oof_names); folds = np.concatenate(oof_folds)\n","auc = roc_auc_score(true,oof)\n","print('Overall OOF AUC with TTA = %.3f'%auc)\n","\n","# SAVE OOF TO DISK\n","df_oof = pd.DataFrame(dict(\n","    image_name = names, target=true, pred = oof, fold=folds))\n","df_oof.to_csv('oof.csv',index=False)\n","df_oof.head()\"\"\""],"metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Step 5: Post process\n","There are ways to modify predictions based on patient information to increase CV LB. You can experiment with that here on your OOF."],"metadata":{}},{"cell_type":"markdown","source":["# Submit To Kaggle"],"metadata":{}},{"cell_type":"code","source":["ds = get_dataset(files_test, augment=False, repeat=False, dim=IMG_SIZES[fold],\n","                 labeled=False, return_image_names=True)\n","\n","image_names = np.array([img_name.numpy().decode(\"utf-8\") \n","                        for img, img_name in iter(ds.unbatch())])"],"metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["submission = pd.DataFrame(dict(image_name=image_names, target=preds[:,0]))\n","submission = submission.sort_values('image_name') \n","submission.to_csv('submission.csv', index=False)\n","submission.head()"],"metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plt.hist(submission.target,bins=100)\n","plt.show()"],"metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}